{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled75.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPCPSLFjVPJZA7ClWT2ctAA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwankang/start-RL/blob/master/gridworld_value_iteration.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRJqXxPMmHnV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "BOARD_ROWS = 3 \n",
        "BOARD_COLS = 4 \n",
        "WIN_STATE = (0, 3) # +1의 보상을 가지는 종단 상태 위치\n",
        "LOSE_STATE = (1, 3) # -1의 보상을 가지는 종단 상태 위치\n",
        "BLOCKED_STATE = (1, 1) # 이동할 수 없는 영역\n",
        "START = (2, 0) # 시작 상태 위치\n",
        "DETERMINISTIC = False # 상태 전이 함수의 확률을 적용하기 위한 플래그. False일 경우에 적용.\n",
        "\n",
        "class State:\n",
        "    def __init__(self, state = START):\n",
        "        self.state = state\n",
        "        self.isEnd = False\n",
        "        self.determine = DETERMINISTIC\n",
        "\n",
        "    def giveReward(self):\n",
        "        if self.state == WIN_STATE:\n",
        "            return 1\n",
        "        elif self.state == LOSE_STATE:\n",
        "            return -1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def isEndFunc(self):\n",
        "        if (self.state == WIN_STATE) or (self.state == LOSE_STATE):\n",
        "            self.isEnd = True\n",
        "\n",
        "    def _chooseActionProb(self, action):\n",
        "        if action == \"U\":\n",
        "            return np.random.choice([\"U\", \"L\", \"R\"], p = [0.8, 0.1, 0.1])\n",
        "        if action == \"D\":\n",
        "            return np.random.choice([\"D\", \"L\", \"R\"], p = [0.8, 0.1, 0.1])\n",
        "        if action == \"L\":\n",
        "            return np.random.choice([\"L\", \"U\", \"D\"], p = [0.8, 0.1, 0.1])\n",
        "        if action == \"R\":\n",
        "            return np.random.choice([\"R\", \"U\", \"D\"], p = [0.8, 0.1, 0.1])\n",
        "    \n",
        "    # 격자 공간 내에서의 다음 상태를 반환\n",
        "    def nxtPosition(self, action):\n",
        "        if self.determine:\n",
        "            if action == \"U\":\n",
        "                nxtState = (self.state[0] - 1, self.state[1])\n",
        "            elif action == \"D\":\n",
        "                nxtState = (self.state[0] + 1, self.state[1])\n",
        "            elif action == \"L\":\n",
        "                nxtState = (self.state[0], self.state[1] - 1)\n",
        "            else:\n",
        "                nxtState = (self.state[0], self.state[1] + 1)\n",
        "            self.determine = False\n",
        "        else:\n",
        "            # 상태 전이 함수를 적용\n",
        "            action = self._chooseActionProb(action)\n",
        "            self.determine = True\n",
        "            nxtState = self.nxtPosition(action)\n",
        "\n",
        "        # 벽을 뚫거나, 이동할 수 없는 영역으로 상태를 바꿀 수 없음\n",
        "        if (nxtState[0] >= 0) and (nxtState[0] <= 2):\n",
        "            if (nxtState[1] >= 0) and (nxtState[1] <= 3):\n",
        "                if nxtState != BLOCKED_STATE:\n",
        "                    return nxtState\n",
        "        return self.state\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.states = []  # 위치와 행동을 기록 record position and action taken at the position\n",
        "        self.actions = [\"U\", \"D\", \"L\", \"R\"]\n",
        "        self.State = State()\n",
        "        self.isEnd = self.State.isEnd\n",
        "        self.lr = 0.2\n",
        "        self.decay_gamma = 0.9 # 할인율은 0.9로 설정\n",
        "\n",
        "        # 전체 상태에 대해 Q함수(행동 가치 함수) 값 초기화\n",
        "        self.Q_values = {}\n",
        "        for i in range(BOARD_ROWS):\n",
        "            for j in range(BOARD_COLS):\n",
        "                self.Q_values[(i, j)] = {}\n",
        "                for a in self.actions:\n",
        "                    self.Q_values[(i, j)][a] = 0  # Q value is a dict of dict\n",
        "\n",
        "    # Q값을 가장 극대화시키는 방향으로 다음 행동을 선택\n",
        "    def chooseAction(self):\n",
        "        max_nxt_reward = 0\n",
        "        action = \"\"\n",
        "\n",
        "        for a in self.actions:\n",
        "            current_position = self.State.state\n",
        "            nxt_reward = self.Q_values[current_position][a]\n",
        "            if nxt_reward >= max_nxt_reward:\n",
        "                action = a\n",
        "                max_nxt_reward = nxt_reward\n",
        "            #print(\"current pos: {}, greedy aciton: {}\".format(self.State.state, action))\n",
        "        return action\n",
        "\n",
        "    # 행동 후 상태 업데이트\n",
        "    def takeAction(self, action):\n",
        "        position = self.State.nxtPosition(action)   \n",
        "        return State(state = position)\n",
        "\n",
        "    # 종단 상태 도달 후 에피소드 초기화\n",
        "    def reset(self):\n",
        "        self.states = []\n",
        "        self.State = State()\n",
        "        self.isEnd = self.State.isEnd\n",
        "\n",
        "    # 에피소드 개수만큼 반복\n",
        "    def play(self, episodes = 10):\n",
        "        i = 0\n",
        "        while i < episodes:\n",
        "            # to the end of game back propagate reward\n",
        "            if self.State.isEnd:\n",
        "                # back propagate\n",
        "                reward = self.State.giveReward()\n",
        "                for a in self.actions:\n",
        "                    self.Q_values[self.State.state][a] = reward\n",
        "                #print(\"Game End Reward\", reward)\n",
        "                for s in reversed(self.states):\n",
        "                    current_q_value = self.Q_values[s[0]][s[1]]\n",
        "                    reward = current_q_value + self.lr * (self.decay_gamma * reward - current_q_value)\n",
        "                    self.Q_values[s[0]][s[1]] = round(reward, 3)\n",
        "                self.reset()\n",
        "                i += 1\n",
        "            else:\n",
        "                action = self.chooseAction()\n",
        "                # append trace\n",
        "                self.states.append([(self.State.state), action])\n",
        "                #print(\"current position {} action {}\".format(self.State.state, action))\n",
        "                # by taking the action, it reaches the next state\n",
        "                self.State = self.takeAction(action)\n",
        "                # mark is end\n",
        "                self.State.isEndFunc()\n",
        "                #print(\"nxt state\", self.State.state)\n",
        "                #print(\"---------------------\")\n",
        "                self.isEnd = self.State.isEnd\n",
        "\n",
        "ag = Agent()\n",
        "\n",
        "ag.play(1000)\n",
        "print(\"latest Q-values ... \\n\")\n",
        "print(ag.Q_values)"
      ]
    }
  ]
}